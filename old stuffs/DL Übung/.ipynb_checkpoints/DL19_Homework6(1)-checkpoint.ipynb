{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ol6-Fgcp2DgQ"
   },
   "source": [
    "# Exercise Sheet 6 – Natural Language Processing with BERT \n",
    "\n",
    " * Deep Learning – Winter term 2019/20\n",
    " * Instructor: Alexander Ecker\n",
    " * Tutor: Lindrit Kqiku <kqiku@cs.uni-goettingen.de>\n",
    " * Due date: Feb 3, 2020 at noon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LZp4BzDU_2Ig"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "The aim of this exercise is to build a model that can read textual data and make predictions about the *sentiment* of that text. We will use the IMDB movie data set to design and implement a model that is able to differentiate between positive and negative reviews of a movie.\n",
    "\n",
    "By completing this exercise, you will be able to\n",
    "- Understand and apply transfer learning techniques in NLP\n",
    "- Use the state-of-the-art embedding techniques as part of the embedding layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WVlgjZoP11i8"
   },
   "source": [
    "# IMPORTANT SUBMISSION INSTRUCTIONS\n",
    "\n",
    "- When you're done, download the notebook and rename it to \\<surname1\\>_\\<surname2\\>_\\<surname3\\>.ipynb\n",
    "- Only submit the ipynb file, no other file is required\n",
    "- Submit only once\n",
    "- The deadline is strict\n",
    "- You are required to present your solution in the tutorial; submission of the notebook alone is not sufficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E0LCu5AP16--"
   },
   "source": [
    "# Setup and Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Le0E49B2Ubn"
   },
   "source": [
    "## Using Colab GPU for Training \n",
    "\n",
    "A GPU can be added by going to the menu and selecting \n",
    "\n",
    "\n",
    "```\n",
    "Edit --> Notebook Settings --> Hardware accelerator --> (GPU)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "id": "kfgQWzyZS3zN",
    "outputId": "a7b452ff-8f8c-4f5c-f077-ae1699905124"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n",
      "There is/are 1 GPU(s) available.\n",
      "\n",
      "We will use the GPU:  Tesla T4\n",
      "with the following properties: \n",
      "_CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15079MB, multi_processor_count=40)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There is/are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print()\n",
    "    print(\"We will use the GPU: \", torch.cuda.get_device_name(0))\n",
    "    print(\"with the following properties: \")\n",
    "    print(torch.cuda.get_device_properties(0))\n",
    "else:\n",
    "    print('No GPU available, training on CPU instead.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3UwkbgUn2fnn"
   },
   "source": [
    "## Installing the Hugging Face Library \n",
    "\n",
    " Next, let’s install the transformers package from Hugging Face which will give us a pytorch interface for working with implementations of state-of-the-art embedding layers. This library contains interfaces for  pretrained language models like BERT, XLNet, OpenAI’s GPT and GPT-2. \n",
    "\n",
    "**More details about BERT**\n",
    ">[Github page of the library](https://github.com/huggingface/transformers)\n",
    "\n",
    ">\n",
    "\n",
    "\n",
    ">[Paper](https://arxiv.org/abs/1910.03771v3)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 722
    },
    "colab_type": "code",
    "id": "hOrc3Hf6213A",
    "outputId": "1eb7d21d-cb61-4133-da97-462bad523bd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/fc/bd726a15ab2c66dc09306689d04da07a3770dad724f0883f0a4bfb745087/transformers-2.4.1-py3-none-any.whl (475kB)\n",
      "\r",
      "\u001b[K     |▊                               | 10kB 23.9MB/s eta 0:00:01\r",
      "\u001b[K     |█▍                              | 20kB 6.0MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 30kB 8.5MB/s eta 0:00:01\r",
      "\u001b[K     |██▊                             | 40kB 5.6MB/s eta 0:00:01\r",
      "\u001b[K     |███▍                            | 51kB 6.8MB/s eta 0:00:01\r",
      "\u001b[K     |████▏                           | 61kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |████▉                           | 71kB 9.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████▌                          | 81kB 10.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████▏                         | 92kB 11.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████▉                         | 102kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 112kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████▎                       | 122kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 133kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▋                      | 143kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▎                     | 153kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 163kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▊                    | 174kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▍                   | 184kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 194kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 204kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▌                 | 215kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 225kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▉                | 235kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▌               | 245kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▏              | 256kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 266kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 276kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 286kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 296kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▋           | 307kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▍          | 317kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 327kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▊         | 337kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 348kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 358kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▉       | 368kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 378kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 389kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 399kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 409kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▎   | 419kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 430kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▋  | 440kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▎ | 450kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 460kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▊| 471kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 481kB 9.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.47)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
      "\u001b[K     |████████████████████████████████| 870kB 61.8MB/s \n",
      "\u001b[?25hCollecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 44.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Collecting tokenizers==0.0.11\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/36/7af38d572c935f8e0462ec7b4f7a46d73a2b3b1a938f50a5e8132d5b2dc5/tokenizers-0.0.11-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1MB 52.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.47)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (2.6.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (0.15.2)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884629 sha256=a8b2c0ec708e4697466e7d8e1840b41a9d21c442d6dced2c83995f8a3cba4caa\n",
      "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
      "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.0.11 transformers-2.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fwZZvZAc48Vi"
   },
   "source": [
    "Getting the data can be done automatically or manually.\n",
    "\n",
    "## Downloading Datasets and Creating Folders\n",
    "\n",
    "Execute the following cells and it will automatically run a Python and shell script to download datasets needed to complete this task and create data folders. *NOTE: It is not relevant for our exercise to understand this section' code*\n",
    "\n",
    "If you are using Google Colab just execute the cells below. \n",
    "In the other case, please follow these steps:\n",
    "1.   Create a python script named download.py and copy-paste the next cells below. Note: You will need to install requests library (pip install requests)\n",
    "2.   Create a shell script by copy-pasting next cell and execute it\n",
    "\n",
    "\n",
    "\n",
    "Alternatively, you can follow the link below and create the directory structure by yourself\n",
    "\n",
    "> Save the data: [Imdb Data set](https://drive.google.com/file/d/13RliAESnCKvPA7_6PUUBYqUJB_ecMp7s/view?usp=sharing) in the folder: /content/data/imdb-ds-rating\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "colab_type": "code",
    "id": "WXSD435f5Zyv",
    "outputId": "f260bb4a-7ef2-4a5d-97c8-20238a30b745"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.21.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%shell\n",
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "KRR9J9gd6e7q",
    "outputId": "7bf6b99f-f410-43c0-9899-5cc9231d6b18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing download.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile download.py\n",
    "# Creates a python script named download.py to download our datasets \n",
    "# from google drive files\n",
    "#\n",
    "# CREDITS: [1] https://stackoverflow.com/a/39225039\n",
    "#          [2] Natural Language Processing with PyTorch - Build Intelligent Language Applications Using Deep Learning - Delip Rao & Brian McMahan\n",
    "\n",
    "\n",
    "import requests\n",
    "def progress_bar(some_iter):\n",
    "    try:\n",
    "        from tqdm import tqdm\n",
    "        return tqdm(some_iter)\n",
    "    except ModuleNotFoundError:\n",
    "        return some_iter\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    print(\"Downloading {}\".format(destination))\n",
    "\n",
    "    def get_confirm_token(response):\n",
    "        for key, value in response.cookies.items():\n",
    "            if key.startswith('download_warning'):\n",
    "                return value\n",
    "\n",
    "        return None\n",
    "\n",
    "    def save_response_content(response, destination):\n",
    "        CHUNK_SIZE = 32768\n",
    "\n",
    "        with open(destination, \"wb\") as f:\n",
    "            for chunk in progress_bar(response.iter_content(CHUNK_SIZE)):\n",
    "                if chunk: # filter out keep-alive new chunks\n",
    "                    f.write(chunk)\n",
    "\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Usage: python download.py drive_file_id destination_file_path\")\n",
    "    else:\n",
    "        # TAKE ID FROM SHAREABLE LINK\n",
    "        file_id = sys.argv[1]\n",
    "        # DESTINATION FILE ON YOUR DISK or CLOUD\n",
    "        destination = sys.argv[2]\n",
    "        download_file_from_google_drive(file_id, destination)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Kb0fOBG36f8G",
    "outputId": "524f3dcd-912a-49f8-916b-6316da25bd69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /content/data/imdb-ds/IMDB_Dataset.npz\n",
      "1985it [00:00, 2006.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%shell\n",
    "#! /bin/bash\n",
    "\n",
    "# For each file, a download.py line is added to call the previous script\n",
    "# Any additional processing on the downloaded file\n",
    "HERE=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" && pwd )\"\n",
    "\n",
    "# IMDB Reviews Dataset\n",
    "mkdir -p $HERE/data/imdb-ds\n",
    "if [ ! -f $HERE/data/imdb-ds/IMDB_Dataset.npz ]; then\n",
    "    python download.py 10qLP8pckM_oTs2cqN48Km-whkOBBXj4c $HERE/data/imdb-ds/IMDB_Dataset.npz\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "iagHM_62E5Uh",
    "outputId": "a146d82e-62dd-42ed-aa06-3cf7224d8ff2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 63M\n",
      "drwxr-xr-x 2 root root 4.0K Jan 31 20:56 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
      "drwxr-xr-x 3 root root 4.0K Jan 31 20:55 \u001b[01;34m..\u001b[0m/\n",
      "-rw-r--r-- 1 root root  63M Jan 31 20:56 IMDB_Dataset.npz\n"
     ]
    }
   ],
   "source": [
    "ls /content/data/imdb-ds/ -alh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ztqFJHCTHHe0"
   },
   "outputs": [],
   "source": [
    "# !rm /content/data/imdb-ds/IMDB_Dataset.npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hu-hKzYgFvZJ"
   },
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hDiG-AxX6tQd"
   },
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "it5JOetS6sgI"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kXM_-H-m7B-B"
   },
   "outputs": [],
   "source": [
    "# load the training data\n",
    "file_name = '/content/data/imdb-ds/IMDB_Dataset.npz'\n",
    "data = np.load(file_name, allow_pickle=True)\n",
    "reviews, labels = data['sentences'], data['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n7ATmijoCrLM"
   },
   "source": [
    "### TODO: Explore the dataset\n",
    "- What is its shape\n",
    "- is the dataset balanced?\n",
    "- Visualize the length of the reviews for some of the reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "dhHLq68PCn2-",
    "outputId": "1cc15cf1-c27c-4779-d862-61d6957fb201"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "50000\n",
      "50000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "962"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(data))\n",
    "print(reviews.size)\n",
    "print(labels.size)\n",
    "\n",
    "#balanced mean same number of pos and neg reviews in this case\n",
    "sum(labels) #one is encoded with one and the other with zero\n",
    "#half is pos and the other half is neg\n",
    "\n",
    "len(reviews[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8g5OyMQPFfKQ"
   },
   "source": [
    "\n",
    "## BERT: Tokenization & Input Formatting\n",
    "\n",
    "We will use the excellent [Transformers library](https://github.com/huggingface/transformers) by Hugging Face to work with BERT.\n",
    "\n",
    "The first step is to tokenize the reviews and bring them into the format that BERT expects. This includes\n",
    "\n",
    "- Tokenization\n",
    "- Adding special tokens: [CLS], [SEP]\n",
    "- Trimming sentences to maximum length\n",
    "- Padding [PAD] in case of shorter sentences\n",
    "\n",
    "Documentation of `BertTokenizer`: https://huggingface.co/transformers/model_doc/bert.html#berttokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "cd48d7a7d18d417e9da10bf43b95a6c7",
      "8fd709ec0e8243f3823bf5e0741bbb22",
      "7f2fcdec57244aa782d4c4bac97a9fa2",
      "e028feaf72e24784a9114e3436f57e64",
      "5293ccc561974aa4832bf2e50ad128f0",
      "24e2be7b49444b32bd47da4476bfe4d1",
      "ef1a0dcf86944f26918dfee38004abcc",
      "56fa1607dcbc40cf84e00f2bb7c79ccb"
     ]
    },
    "colab_type": "code",
    "id": "eZTmDwOueqEN",
    "outputId": "d7317441-3988-4569-d107-1eebd8ec5ba3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the BERT tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd48d7a7d18d417e9da10bf43b95a6c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get rid of Colab warning about Tensorflow 2.0\n",
    "%tensorflow_version 1.x\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading the BERT tokenizer...')\n",
    "\n",
    "# We will use bert-base-uncased model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "EM0eJoNaKFXb",
    "outputId": "4c9ccf8a-a345-44ae-da0f-aedd37772f64"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [03:56<00:00, 211.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# Maximum length of a sequence\n",
    "MAX_LEN = 128\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "# For every sentence...\n",
    "for review in tqdm(reviews):\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad to maximum length if the sequence is shorter\n",
    "    sequence = tokenizer.encode_plus(\n",
    "                    review,                      # Review to encode.\n",
    "                    add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                    max_length=MAX_LEN,\n",
    "                    pad_to_max_length=True,\n",
    "    )\n",
    "    input_ids.append(sequence['input_ids'])\n",
    "\n",
    "input_ids = np.array(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 259
    },
    "colab_type": "code",
    "id": "D9P8lwIiMHLv",
    "outputId": "fba684d9-1627-49e4-c495-a502c6344989"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Phil the Alien is one of those quirky films where the humour is based around the oddness of everything rather than actual punchlines.At first it was very odd and pretty funny but as the movie progressed I didn't find the jokes or oddness funny anymore.Its a low budget film (thats never a problem in itself), there were some pretty interesting characters, but eventually I just lost interest.I imagine this film would appeal to a stoner who is currently partaking.For something similar but better try \"Brother from another planet\"\n",
      "Token IDs: [  101  6316  1996  7344  2003  2028  1997  2216 21864 15952  3152  2073\n",
      "  1996 17211  2003  2241  2105  1996  5976  2791  1997  2673  2738  2084\n",
      "  5025  8595 12735  1012  2012  2034  2009  2001  2200  5976  1998  3492\n",
      "  6057  2021  2004  1996  3185 12506  1045  2134  1005  1056  2424  1996\n",
      " 13198  2030  5976  2791  6057  4902  1012  2049  1037  2659  5166  2143\n",
      "  1006  2008  2015  2196  1037  3291  1999  2993  1007  1010  2045  2020\n",
      "  2070  3492  5875  3494  1010  2021  2776  1045  2074  2439  3037  1012\n",
      "  1045  5674  2023  2143  2052  5574  2000  1037  2962  2099  2040  2003\n",
      "  2747  2112 15495  1012  2005  2242  2714  2021  2488  3046  1000  2567\n",
      "  2013  2178  4774  1000   102     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "# Print example sentence 10, now as a list of IDs.\n",
    "i = 10\n",
    "print('Original: ', reviews[i])\n",
    "print('Token IDs:', input_ids[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3qQYZFU0VXhI"
   },
   "source": [
    "## Training, Validation, Test Split\n",
    "\n",
    "Use 1000-2000 reviews as validation and test set, respectively. Use the rest for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "3E58xzigaBA_",
    "outputId": "dad8aecd-0a76-49d6-e832-4b76bf8c72a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46000, 128) (46000,)\n",
      "(2000, 128) (2000,)\n",
      "(2000, 128) (2000,)\n"
     ]
    }
   ],
   "source": [
    "#also erst 4000 zufallszahlen zwischen 0 und 49999 (einschließlich) erzeugen und dann am Ende einfach in Test und Val teilen\n",
    "#Alle Zahlen die nicht darin sind, sind dann im Train\n",
    "\n",
    "## ToDo: Split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "import random\n",
    "random.seed(1234) #for recapabebility\n",
    "num = random.sample(range(50000), k=4000)\n",
    "mask = np.ones(50000, dtype=bool)\n",
    "mask[num] = False\n",
    "\n",
    "train_features, train_labels = input_ids[mask,...], labels[mask,...]\n",
    "test_features, test_labels = input_ids[num[0:2000]], labels[num[0:2000]]\n",
    "val_features, val_labels = input_ids[num[2000:]], labels[num[2000:]]\n",
    "\n",
    "\n",
    "## ToDo: Print out the shapes of your splitted train, validation, and test set\n",
    "print(train_features.shape, train_labels.shape)\n",
    "print(test_features.shape, test_labels.shape)\n",
    "print(val_features.shape, val_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NaSTkvQca3or"
   },
   "source": [
    "## Create DataLoader\n",
    "\n",
    "You can use `TensorDataset` to create a dataset holding (reviews, labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Uhhy1VhJMZV"
   },
   "outputs": [],
   "source": [
    "#überprüfen, ob die eckigen Klammern benötigt werden\n",
    "train_features = torch.tensor([train_features])\n",
    "train_labels = torch.tensor([train_labels])\n",
    "\n",
    "test_features = torch.tensor([test_features])\n",
    "test_labels = torch.tensor([test_labels])\n",
    "\n",
    "val_features = torch.tensor([val_features])\n",
    "val_labels = torch.tensor([val_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ASR4GMK19O3_"
   },
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "\n",
    "#train_dataset = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "val_dataset = torch.utils.data.TensorDataset(val_features, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7my-m1Zt75Zp"
   },
   "outputs": [],
   "source": [
    "# Define the data loaders\n",
    "batch_size = 75\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader   = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader    = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Save data set sizes for later\n",
    "train_size    = len(train_dataset)\n",
    "test_size     = len(test_dataset)\n",
    "val_size      = len(val_dataset)\n",
    "train_batches = len(train_loader)\n",
    "test_batches  = len(test_loader)\n",
    "val_batches   = len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8vuwkaCRkYLL"
   },
   "source": [
    "\n",
    "# Sentiment Network - Classification Model\n",
    "\n",
    "We will build a sentiment classifier by using transfer learning from a pre-trained BERT model. For simplicity and to speed up the training process, we will freeze the weights of the BERT encoder (code already included below).\n",
    "\n",
    "Use the 768-dimensional embedding corresponding to the [CLS] token and add a binary classifier on top. Thus, your linear layer should map a (BATCH_SIZE x 768) tensor onto (BATCH_SIZE, ) or (BATCH_SIZE, 2), depending on whether you use a sigmoid activation and scalar outputs or code the two classes one-hot and use a softmax layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AeJdGstbetUX"
   },
   "source": [
    "### Implement a Binary Classifier \n",
    "\n",
    "Look up how to instantiate a pre-trained `BertModel`. A good starting point is the quickstart guide of the Transformers library: https://huggingface.co/transformers/quickstart.html\n",
    "\n",
    "Documentation of `BertModel`: https://huggingface.co/transformers/model_doc/bert.html#bertmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UqgDnbrl_gny"
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, fine_tune=False):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "\n",
    "        # TODO: BertModel as an embedding layer\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Turn gradients for BertModel on/off\n",
    "        self.bert.requires_grad_(fine_tune)\n",
    "\n",
    "        # TODO: Linear binary classification layer\n",
    "        self.Layer = nn.Linear(x.shape[0]*768,x.shape[0])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        x = self.bert.forward(x)\n",
    "        x = self.Layer(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IOT3lpt2PQNp"
   },
   "source": [
    "## Instantiate the Network \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BRSIccYrIJSs"
   },
   "outputs": [],
   "source": [
    "# ToDo: Instantiate the model\n",
    "net = SentimentClassifier()\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GTERZi4LkdJe"
   },
   "source": [
    "## Training and Testing\n",
    "\n",
    "Make sure to choose an appropriate learning rate. Regularly output training and validation loss + accuracy. Since the dataset is large, you want to output statistics in regular intervals also within an epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ipI-UlSskm13"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "loss = \n",
    "\n",
    "# ToDo: Train & validate\n",
    "def train(model, train_data):\n",
    "  train_loss = []\n",
    "  for x,y in train_data:\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x = x.to(device)\n",
    "    y_hat = model(x)\n",
    "\n",
    "    loss = loss(y_hat, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_loss.append(loss.item())\n",
    "  \n",
    "  return train_loss\n",
    "\n",
    "def validate(model, val_data):\n",
    "  val_loss = []\n",
    "  for x,y in val_data:\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "feVzyzZkWN3l"
   },
   "source": [
    "Plot the training and validation loss and report accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9yhhzoIVWK2n"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "us5O1pi6tQBl"
   },
   "source": [
    "## Inference Testing\n",
    "\n",
    "This section consists of testing the models using inference testing, i.e., in our case, using other reviews to categorize them as positive or negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lZLcwRs6tVdV"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "## Create a function that takes in the trained net, a text review, \n",
    "## and a sequence length to predict its sentiment to positive/negative\n",
    "\n",
    "def predict(net, test_review, sequence_length=MAX_LEN):\n",
    "        '''\n",
    "        params:\n",
    "        net - The trained net \n",
    "        test_review - a review as a string\n",
    "        sequence_length - the padded length of a review\n",
    "        '''\n",
    "    \n",
    "    \n",
    "    # print custom response based on whether test_review is pos/neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BSDQ3xG_uWFw"
   },
   "outputs": [],
   "source": [
    "# TODO: Choose any of your preferred movies, find a review from imdb.com or\n",
    "#       any other source and compare your model result against the source rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MWb3P4SWUD3C"
   },
   "source": [
    "## [Optional] Fine-tune BERT model\n",
    "\n",
    "Can you improve performance by fine-tuning BERT in addition to training the linear classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8m-Up4tDUBqu"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DL19_Homework6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "24e2be7b49444b32bd47da4476bfe4d1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5293ccc561974aa4832bf2e50ad128f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "56fa1607dcbc40cf84e00f2bb7c79ccb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f2fcdec57244aa782d4c4bac97a9fa2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_24e2be7b49444b32bd47da4476bfe4d1",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5293ccc561974aa4832bf2e50ad128f0",
      "value": 231508
     }
    },
    "8fd709ec0e8243f3823bf5e0741bbb22": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd48d7a7d18d417e9da10bf43b95a6c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7f2fcdec57244aa782d4c4bac97a9fa2",
       "IPY_MODEL_e028feaf72e24784a9114e3436f57e64"
      ],
      "layout": "IPY_MODEL_8fd709ec0e8243f3823bf5e0741bbb22"
     }
    },
    "e028feaf72e24784a9114e3436f57e64": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_56fa1607dcbc40cf84e00f2bb7c79ccb",
      "placeholder": "​",
      "style": "IPY_MODEL_ef1a0dcf86944f26918dfee38004abcc",
      "value": "100% 232k/232k [00:00&lt;00:00, 889kB/s]"
     }
    },
    "ef1a0dcf86944f26918dfee38004abcc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
