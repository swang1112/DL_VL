{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z0qdA8OhP8PH"
   },
   "source": [
    "# Exercise Sheet 3 - Convolutional Neural Networks on MNIST + Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z5ZPkPLkQOZ-"
   },
   "source": [
    " * Deep Learning â€“ Winter term 2019/20\n",
    " * Instructor: Prof. Dr. Alexander Ecker\n",
    " * Tutors: Pronaya Prosun Das, Samaneh Sadegh and Muhammad Jazib Zafar\n",
    " * Due date: Jan 13, 2020 at noon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jqwsYptpP-Kf"
   },
   "source": [
    "In this assignment you will learn how to train a Convolutional Neural Network to classify images. We will work with the MNIST hand-written digits dataset.\n",
    "The goals of this assignment are as follows:\n",
    "\n",
    "*   Exploring the architecture of CNNs like number of features, kernel sizes and pooling.\n",
    "*   Understanding the impact of batch normalization.\n",
    "*   Creating custom `nn.Module` in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K72L2EwHQBmj"
   },
   "source": [
    "### IMPORTANT SUBMISSION INSTRUCTIONS\n",
    "\n",
    "- When you're done, download the notebook and rename it to \\<surname1\\>_\\<surname2\\>_\\<surname3\\>.ipynb\n",
    "- Only submit the ipynb file, no other file is required\n",
    "- Submit only once\n",
    "- The deadline is strict\n",
    "- You are required to present your solution in the tutorial; submission of the notebook alone is not sufficient\n",
    "\n",
    "Implementation\n",
    "- Only change code to replace placeholders. Leave the other code as is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AtuI4d3lQFJ8"
   },
   "source": [
    "\n",
    "\n",
    "### **PART 1**\n",
    "\n",
    "**Importing required libraries.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cb_HlnJVQPoK"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as ds\n",
    "import torchvision.transforms as T\n",
    "import pathlib\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N7JinRHlQUlX"
   },
   "source": [
    "**load dataset.**\n",
    "\n",
    "We use the MNIST dataset. This might take a couple minutes the first time you do it. Use appropriate training and validation samples.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1TetH-_-QnvT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch._C.Generator object at 0x000002959BABE030>\n"
     ]
    }
   ],
   "source": [
    "#seed is important for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "print(torch.manual_seed(seed))\n",
    "\n",
    "mnist_transforms = T.Compose([T.ToTensor(), T.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "batch_size = 256\n",
    "# Load MNIST dataset\n",
    "mnist_trainset = ds.MNIST(root='./data', train=True, download=True, transform=mnist_transforms)\n",
    "train_size = int(0.8 * len(mnist_trainset))\n",
    "val_size = len(mnist_trainset) - train_size\n",
    "train_set, val_set = torch.utils.data.random_split(mnist_trainset, [train_size, val_size])\n",
    "\n",
    "trainloader = DataLoader(train_set, batch_size=batch_size,\n",
    "                         shuffle=True, num_workers=2)\n",
    "valloader = DataLoader(val_set, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=2)\n",
    "mnist_testset = ds.MNIST(root='./data', train=False, download=True, transform=mnist_transforms)\n",
    "testloader = DataLoader(mnist_testset, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "phd-1udQQyj3"
   },
   "source": [
    "**Define a model.**\n",
    "\n",
    "The first step to training a model is defining its architecture. \n",
    "Use `nn.Sequential` to define a model with following structure:\n",
    "![Imgur](https://i.imgur.com/7LfRN2y.jpg)\n",
    "*   Convolutional layer with 32 filters, kernel size of 5*5 and stride of 1.\n",
    "*   Max Pooling layer with kernel size of 2*2 and default stride (2).\n",
    "*   ReLU activation function.\n",
    "*   Linear layer with output of 512.\n",
    "*   ReLU activation function.\n",
    "*   A linear layer with output of 10.\n",
    "*   At the end put a softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NQA61-30RilS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MOD1()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "class MOD1(nn.Module): \n",
    "    def _init_(self):\n",
    "        super()._init_()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, stride=1, filters=32) #nn.Conv2d(input chanels, output chanels, kernel)\n",
    "        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "        #output shape: M=(N-k)/s+1=(28-5)/1+1=23+1=24\n",
    "        #out_channels= 32 because of 32 filters!\n",
    "        self.layer1 = nn.Linear(12*12*32,512) #input_Size maybe 12*12*32 because of the image size and the number of filters, but what ist the impact of the pooling \n",
    "        self.layer2 = nn.Linear(512,10)        \n",
    "        self.sll = nn.Sequential(\n",
    "            self.conv1(),\n",
    "            nn.functional.max_pool2d(32, (2,2)),\n",
    "            nn.functional.ReLU(),\n",
    "            self.layer1(),\n",
    "            nn.functional.ReLU(),\n",
    "            self.layer2(),\n",
    "            nn.functional.Softmax() #maybe better use LogSoftmax, because later we should use the cross.entropy\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.sll(x)\n",
    "        return x\n",
    "        \n",
    "    \"\"\"In nn.Sequential, the nn.Module's stored inside are connected in a cascaded way. For instance, \n",
    "    in the example that I gave, I define a neural network that receives as input an image with 3 channels and \n",
    "    outputs 10 neurons. That network is composed by the following blocks, in the following order: Conv2D -> \n",
    "    ReLU -> Linear layer. Moreover, an object of type nn.Sequential has a forward() method, so if I have an \n",
    "    input image x I can directly call y = simple_cnn(x) to obtain the scores for x. When you define an \n",
    "    nn.Sequential you must be careful to make sure that the output size of a block matches the input size of \n",
    "    the following block. Basically, it behaves just like a nn.Module\"\"\"\n",
    "    \n",
    "model = MOD1()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-b8e0565c5231>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./mnist_classifier_model.pth\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./mnist_classifier_model.pth\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(32 * 12 * 12, 512)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.pool(self.conv1(x)))\n",
    "        # Changing view because for executing the 3d-tensor as a 1d tensor in the linear layers\n",
    "        x = x.view(x.shape[0], 32 * 12 * 12)\n",
    "        x = self.relu2(self.fc1(x))\n",
    "        x = self.softmax(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "if os.path.exists(\"./mnist_classifier_model.pth\"):\n",
    "    net.load_state_dict(torch.load(\"./mnist_classifier_model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UfgW-igIVX9j"
   },
   "source": [
    "**Train the model**\n",
    "\n",
    "Use the cros-entropy loss and set up an optimizer with appropriate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z6RUJJ3MV0Op"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "loss_fct = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-4, momentum=9e-01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fI-qvYzWWA6W"
   },
   "source": [
    "Train the model using datasets. Run the training for at least 10 epoch. Show validation accuracy and loss in each epoch. Also monitor training accuracy and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I-F5veFQWDgG"
   },
   "outputs": [],
   "source": [
    "def accurancy_fn(correct, total): return float(correct)/total\n",
    "\n",
    "def train(dataloader, model, deviceoptimizer, loss_fct, train_loss):\n",
    "    # TODO\n",
    "    epoch_loss = [] #to store the loss for each epoch\n",
    "    #to store the correct preticeted values and calc the accurancy later, for each epoch seperatly\n",
    "    epoch_correct = 0\n",
    "    epoch_total = 0 \n",
    "    #loop over the data\n",
    "    for x, y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        model.train()\n",
    "        y_pred = model(x.to(device))\n",
    "        hits = y.to(device) == y_pred.argmax(dim=1).to(device)\n",
    "        epoch_correct += sum(hits)\n",
    "        epoch_total += len(hits)\n",
    "        loss = loss_fct(y_pred.zo(device), y.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "        average_loss = sum(epoch_loss[-50:])/len(epoch_loss[-50:])\n",
    "    train_loss.extend(epoch_loss)\n",
    "    epoch_accurancy = accurancy_fn(epoch_correct, epoch_total)\n",
    "    return epoch_loss, epoch_accurancy \n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SdZCUHa4qdJ8"
   },
   "outputs": [],
   "source": [
    "start_time = time.time() \n",
    "(...) = train(...)\n",
    "print(\"--- execution time in seconds : %s ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EiEiPcCkiZYN"
   },
   "source": [
    "**Show plots**\n",
    "\n",
    "Show the Epoch Vs Accuracy and Epoch Vs Validation Loss plot from the previous training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YLZut9gAktj-"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loader_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-093dd87f5607>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# TODO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mloader_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'loader_test' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oe5KCMI2rB5u"
   },
   "source": [
    "**Check the accuracy of the model.**\n",
    "\n",
    "Check the accuracy of the model using test dataset: loader_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lzcDq7iUrOKm"
   },
   "outputs": [],
   "source": [
    "def check_accuracy(data, model):\n",
    "    #nutzen des trainierten Models und dann vergleichen der \n",
    "    #split the data to the images and the output\n",
    "    x, y = data\n",
    "    \n",
    "    #get the size of inputs/ predictions\n",
    "    total = len(y)\n",
    "    #predict the output with the trained model\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    #calculate the number of correct predicted numbers so y==y_pred\n",
    "    #but with respect to the probability, because the model just returns probs for the outputs and not just a singel value\n",
    "    correct = sum(y==y_pred.argmax(dim=1))\n",
    "    return(accurancy(correct, total))\n",
    "    \n",
    "  # TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-15iUW2DqzGl"
   },
   "outputs": [],
   "source": [
    "check_accuracy(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NEvjy5RO3hjE"
   },
   "source": [
    "### **PART 2**\n",
    "\n",
    "We now add batch normalization to the convolutional layer and dropout to the fully-connected layer. Both should improve performance of the model.\n",
    "\n",
    "**Batch Normalization**\n",
    "\n",
    "Normalization is done to adjust and scale the activations. For example, when we have features from 0 to 1 and some from 1 to 1000, we should normalize them to speed up learning. If the input layer is benefiting from it, the same should also hold for the values in the hidden layers. Batch normalization improves training speed and stabilizes training by avoiding vanishing or exploding gradients.\n",
    "\n",
    "**Dropout**\n",
    "\n",
    "Dropout is a regularization method. It temporarily removes a unit from the network, along with all its incoming and outgoing connections. \n",
    "\n",
    "**Task**\n",
    "\n",
    "Add batch normalization after convolutional layer and put a dropout layer with probality 0.6 after the first linear layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_r-at5Nr7qwR"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "#took the model from Part 1\n",
    "class MOD2(nn.Module): \n",
    "    def _init_(self):\n",
    "        super()._init_()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, stride=1, filters=32) #nn.Conv2d(input chanels, output chanels, kernel)\n",
    "        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "        #output shape: M=(N-k)/s+1=(28-5)/1+1=23+1=24\n",
    "        #out_channels= 32 because of 32 filters!\n",
    "        self.bn1 = nn.BatchNorm2d(32) #batchnorm of 32, because of 32 filters\n",
    "        self.layer1 = nn.Linear(12*12*32,512) #input_Size maybe 12*12*32 because of the image size and the number of filters, but what ist the impact of the pooling \n",
    "        self.layer2 = nn.Linear(512,10)        \n",
    "        self.sll = nn.Sequential(\n",
    "            self.conv1(),\n",
    "            self.bn1(),\n",
    "            nn.functional.max_pool2d(32, (2,2)),\n",
    "            nn.functional.ReLU(),\n",
    "            self.layer1(),\n",
    "            nn.Dropout(p=0.6),\n",
    "            nn.functional.ReLU(),\n",
    "            self.layer2(),\n",
    "            nn.functional.Softmax() #maybe better use LogSoftmax, because later we should use the cross.entropy\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.sll(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vF2yo3Hw8kPO"
   },
   "source": [
    "Train the model again with the same loss function and optimizer. Just call the train function you have written earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CxFx7hFb8-Ka"
   },
   "outputs": [],
   "source": [
    "# Your loss function and optimizer\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jyzga8iD9DG_"
   },
   "outputs": [],
   "source": [
    "start_time = time.time() \n",
    "(...) = train(...)\n",
    "print(\"--- execution time in seconds : %s ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_FJsRdG-_QM7"
   },
   "source": [
    "Is there any change in train, validation and test accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ue_8m7j3_X5-"
   },
   "outputs": [],
   "source": [
    "check_accuracy(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jc4uFlHzFd9V"
   },
   "source": [
    "### **PART 3**\n",
    "\n",
    "In this section we implement a model with multiple convolutional layers and train using GPU (if available).\n",
    "\n",
    "Implement the following architecture by subclassing `nn.Module`:\n",
    "\n",
    "![Imgur](https://imgur.com/rpBqY43.png)\n",
    "\n",
    "**Setting Free GPU**\n",
    "\n",
    "Go to Edit > Notebook settings or Runtime > Change runtime type and select GPU as Hardware accelerator.\n",
    "\n",
    "![Imgur](https://imgur.com/wGchqmj.png)\n",
    "\n",
    "For details, please read the \n",
    "[this](https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d)\n",
    "article.\n",
    "\n",
    "Also you have to move all the tensors to the GPU, otherwise it will run on CPU. Train the model for at least 10 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j3FbKt57CPdo"
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class CustomModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        # TODO\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "cnn = CustomModel()\n",
    "\n",
    "# Utilize GPU\n",
    "device = torch.device('cuda:0')\n",
    "cnn = cnn.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FRoud9yxoohx"
   },
   "source": [
    "Define the loss function and optimizer,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m23_T9dWebPN"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ugoF8oxOoy2E"
   },
   "source": [
    "Call the train function you have written earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OvcZaHRPejM6"
   },
   "outputs": [],
   "source": [
    "start_time = time.time() \n",
    "(...) = train(...)\n",
    "print(\"--- execution time in seconds : %s ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "elcAYXsrpGLJ"
   },
   "source": [
    "Show the accuracy by calling check_accuracy(...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sjzZJqyHU6Ej"
   },
   "outputs": [],
   "source": [
    "check_accuracy(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nzgvz7i8Nijd"
   },
   "source": [
    "#### [OPTIONAL]\n",
    "Can you get the performance of the CNN to 99.5% with the above architecture? If not, can you change it to improve performance? Note that state-of-the-art performance on MNIST with CNNs is around 99.8% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RTuvaTYjNi4Y"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment3_TODO.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
