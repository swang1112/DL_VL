{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ms0joNfRzag-"
   },
   "source": [
    "# Exercise Sheet 4 - Transfer Learning for Saliency Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bzBpDwtBzd46"
   },
   "source": [
    " * Deep Learning – Winter term 2019/20\n",
    " * Instructor: Prof. Dr. Alexander Ecker\n",
    " * Tutor: Timo Janßen (<timo.janssen@theorie.physik.uni-goettingen.de>)\n",
    " * Due date: Jan 20, 2020 at noon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IqZXvojiz0D6"
   },
   "source": [
    "In this assignment you will learn how to use transfer learning to predict where people will look in images. We will work with the MIT 1003 dataset which consists of only 1003 images together with fixation data from human subjects. Transfer learning allows us to reuse the features from a model that has been pretrained on a larger dataset. Here we will use VGG19 pretrained on ImageNet.\n",
    "\n",
    "The goals of this assignment are as follows:\n",
    "\n",
    "*   Extract features from a model.\n",
    "*   Implement a simple readout network and train it on the data.\n",
    "*   Define a loss function and a seperate evaluation metric.\n",
    "*   Visualize the best and the worst results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BQ4jpxuU2CqD"
   },
   "source": [
    "### IMPORTANT SUBMISSION INSTRUCTIONS\n",
    "\n",
    "- When you're done, download the notebook and rename it to \\<surname1\\>_\\<surname2\\>_\\<surname3\\>.ipynb\n",
    "- Only submit the ipynb file, no other file is required\n",
    "- Submit only once\n",
    "- The deadline is strict\n",
    "- You are required to present your solution in the tutorial; submission of the notebook alone is not sufficient\n",
    "\n",
    "Implementation\n",
    "- Only change code to replace placeholders. Leave the other code as is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1aEctsMZ2a6Z"
   },
   "source": [
    "**Importing required libraries.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "57JMWl2ZDwaq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dKuT7TZ7D2nM"
   },
   "source": [
    "**Get the data**\n",
    "\n",
    "The MIT 1003 dataset is publicly available at https://people.csail.mit.edu/tjudd/WherePeopleLook/index.html\n",
    "\n",
    "For your convenience there is a shared folder on Google Drive: https://drive.google.com/drive/folders/1GoYw3jp9kWLmHN7qylDS5QTo_ebnaMDD?usp=sharing\n",
    "You can click the link and add the folder to your Drive (it does not count into your disk quota).\n",
    "\n",
    "If you're using Colab, you can uncomment the code in the following cell to mount your Google drive. Otherwise, you need to download the data and enter the path to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "yHd5UFWkEc87",
    "outputId": "28928f05-bc62-4b96-fe70-ace503ed4959"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aW_BuLYjD2na"
   },
   "outputs": [],
   "source": [
    "# This notebook and MIT_1003 folder are assumed to be in the same folder\n",
    "main_path = '/content/drive/My Drive/Uni/Teaching/DL19'\n",
    "# main_path = '<path>/<to>/<notebook>'\n",
    "data_path = os.path.join(main_path, 'MIT_1003')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hstop-eGD2nm"
   },
   "source": [
    "**Prepare the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HHwpWifaDwbJ",
    "outputId": "c0f9e12f-8a4c-4156-c56e-92fb340f4023"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Das System kann den angegebenen Pfad nicht finden: '/content/drive/My Drive/Uni/Teaching/DL19\\\\MIT_1003\\\\ALLSTIMULI'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-e5d7a06d51f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimages_files_all\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ALLSTIMULI'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ALLSTIMULI'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.jpeg'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Total Number of Images: {} (should be 1003)\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages_files_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Das System kann den angegebenen Pfad nicht finden: '/content/drive/My Drive/Uni/Teaching/DL19\\\\MIT_1003\\\\ALLSTIMULI'"
     ]
    }
   ],
   "source": [
    "images_files_all  = [os.path.join(data_path, 'ALLSTIMULI', f) for f in os.scandir(os.path.join(data_path, 'ALLSTIMULI')) if f.path.endswith('.jpeg') ]\n",
    "print (\"Total Number of Images: {} (should be 1003)\".format(len(images_files_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Hw_kppidDwbY",
    "outputId": "79058aa4-9b0d-4278-fca0-ae7ba72cb21e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Train Images: 501\n",
      "Number of Test Images: 502\n"
     ]
    }
   ],
   "source": [
    "# fix the seed for reproducible splitting\n",
    "np.random.seed(1234)\n",
    "train_idx = np.random.choice(len(images_files_all), len(images_files_all)//2, replace=False)\n",
    "test_idx = np.setdiff1d(np.arange(len(images_files_all)), train_idx)\n",
    "\n",
    "images_files = {}\n",
    "images_files['train'] = [images_files_all[i] for i in train_idx]\n",
    "images_files['test'] = [images_files_all[i] for i in test_idx]\n",
    "print (\"Number of Train Images: {}\".format(len(images_files['train'])))\n",
    "print (\"Number of Test Images: {}\".format(len(images_files['test'])))\n",
    "\n",
    "fixation_maps_binary_files = {}\n",
    "fixation_maps_binary_files['train']  = [os.path.join(data_path, 'ALLFIXATIONMAPS', os.path.splitext(os.path.basename(filename))[0] + '_fixPts.jpg') for filename in images_files['train']]\n",
    "fixation_maps_binary_files['test']  = [os.path.join(data_path, 'ALLFIXATIONMAPS', os.path.splitext(os.path.basename(filename))[0] + '_fixPts.jpg') for filename in images_files['test']]\n",
    "fixation_maps_blurred_files = {}\n",
    "fixation_maps_blurred_files['train'] = [os.path.join(data_path, 'ALLFIXATIONMAPS', os.path.splitext(os.path.basename(filename))[0] + '_fixMap.jpg') for filename in images_files['train']]\n",
    "fixation_maps_blurred_files['test'] = [os.path.join(data_path, 'ALLFIXATIONMAPS', os.path.splitext(os.path.basename(filename))[0] + '_fixMap.jpg') for filename in images_files['test']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i_ws-tlaDwcI"
   },
   "source": [
    "**Show an example from the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qc4eAt0FDwcL"
   },
   "outputs": [],
   "source": [
    "image = io.imread(images_files['train'][0])\n",
    "binary_map = io.imread(fixation_maps_binary_files['train'][0])\n",
    "blurred_map = io.imread(fixation_maps_blurred_files['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ODPugZmFDwcS"
   },
   "outputs": [],
   "source": [
    "fixations = np.where(binary_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6nr-EMrZDwcg"
   },
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n",
    "ax1.axis('off')\n",
    "ax1.imshow(image)\n",
    "ax2.axis('off')\n",
    "ax2.imshow(blurred_map, cmap='viridis_r')\n",
    "ax3.axis('off')\n",
    "ax3.imshow(image, alpha=0.5)\n",
    "ax3.scatter(fixations[1], fixations[0], s=1, marker='.', c='b')\n",
    "ax4.axis('off')\n",
    "ax4.imshow(image)\n",
    "ax4.imshow(blurred_map, alpha=0.5, cmap='viridis_r')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zgUxMoe_Dwcv"
   },
   "source": [
    "**Preprocess images and maps**\n",
    "\n",
    "The images have different sizes and orientations. To simplify our task we downscale the images and crop them to a size of 256x256 pixels. At this size, all of the images should fit into memory at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mkasX89_Dwcy"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(images_paths, fixations_binary_paths, fixations_blurred_paths, shape):\n",
    "    transform = torchvision.transforms.Compose([torchvision.transforms.ToPILImage(), \n",
    "                                                    torchvision.transforms.Resize(shape), \n",
    "                                                    torchvision.transforms.CenterCrop(shape)])\n",
    "    \n",
    "    n = len(images_paths)\n",
    "    ims = np.empty((n, shape, shape, 3), dtype=np.int)\n",
    "    fixs_binary = np.empty((n, shape, shape))\n",
    "    fixs_blurred = np.empty((n, shape, shape))\n",
    "\n",
    "    for i, path in enumerate(tqdm(images_paths)):\n",
    "        image = io.imread(path)\n",
    "        image = transform(image)\n",
    "        image = np.array(image, dtype=np.int)\n",
    "        ims[i] = image\n",
    "\n",
    "    for i, path in enumerate(tqdm(fixations_binary_paths)):\n",
    "        fixations_binary = io.imread(path, 0)\n",
    "        fixations_binary = transform(fixations_binary)\n",
    "        fixations_binary = np.array(fixations_binary)\n",
    "        fixs_binary[i] = fixations_binary\n",
    "    \n",
    "    for i, path in enumerate(tqdm(fixations_blurred_paths)):\n",
    "        fixations_blurred = io.imread(path, 0)\n",
    "        fixations_blurred = transform(fixations_blurred)\n",
    "        fixations_blurred = np.array(fixations_blurred)\n",
    "        fixs_blurred[i] = fixations_blurred\n",
    "        \n",
    "    data = {'images': ims, 'fixations_binary': fixs_binary, 'fixations_blurred': fixs_blurred}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pK9DS_N4Dwc9"
   },
   "outputs": [],
   "source": [
    "train_data = preprocess_data(images_files['train'], fixation_maps_binary_files['train'], fixation_maps_blurred_files['train'], 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ze_2XKArD2od"
   },
   "source": [
    "**Save/Load preprocessed data**\n",
    "\n",
    "If you want to, you can save the data on disk, so that you don't have to generate it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vZFLzkVDGg9r"
   },
   "outputs": [],
   "source": [
    "train_data_file = os.path.join(main_path, 'train_data.npy')\n",
    "np.save(train_data_file, train_data)\n",
    "#train_data = np.load(train_data_file, allow_pickle=True)[()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EG7BDv0hDwdH"
   },
   "source": [
    "**Implement a PyTorch dataset for our data**\n",
    "\n",
    "This allows us to access the data conveniently and to use it in a dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a0xgIE_sDwdJ"
   },
   "outputs": [],
   "source": [
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, preprocessed_data, features=None):\n",
    "        self.features = features\n",
    "        self.data = preprocessed_data\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        raw_image = self.data['images'][key]\n",
    "        \n",
    "        normalized_image = raw_image.astype(np.float32) / 255.0 # -> [0, 1]\n",
    "        normalized_image -= np.array([0.485, 0.456, 0.406])     # subtract mean\n",
    "        normalized_image /= np.array([0.229, 0.224, 0.225])     # divide by SD   \n",
    "        normalized_image = normalized_image.transpose(2, 0, 1)  # HWC -> CHW\n",
    "        \n",
    "        fixations_binary = self.data['fixations_binary'][key]\n",
    "        fixations_blurred = self.data['fixations_blurred'][key]\n",
    "\n",
    "        data = {\n",
    "            \"raw_image\": raw_image,\n",
    "            \"normalized_image\": normalized_image,\n",
    "            \"fixations_binary\": fixations_binary,\n",
    "            \"fixations_blurred\": fixations_blurred\n",
    "        }\n",
    "\n",
    "        if self.features is not None:\n",
    "            data[\"features\"] = self.features[key]\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data['images'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qS8hk7GfD2ot"
   },
   "source": [
    "The collate_fn will be used in the dataloader. It converts the batched data into tensors. We use a sparse tensor for the binary fixations because they contain mostly zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3KBNBWMXD2ov"
   },
   "outputs": [],
   "source": [
    "def to_sparse(x):\n",
    "    \"\"\" converts dense tensor x to sparse format \"\"\"\n",
    "    x_typename = torch.typename(x).split('.')[-1]\n",
    "    sparse_tensortype = getattr(torch.sparse, x_typename)\n",
    "\n",
    "    indices = torch.nonzero(x)\n",
    "    if len(indices.shape) == 0:  # if all elements are zeros\n",
    "        return sparse_tensortype(*x.shape)\n",
    "    indices = indices.t()\n",
    "    values = x[tuple(indices[i] for i in range(indices.shape[0]))]\n",
    "    return sparse_tensortype(indices, values, x.size())\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch_data = {\n",
    "        \"image\": torch.tensor([item[\"normalized_image\"] for item in batch]),\n",
    "        \"fixations_binary\": to_sparse(torch.tensor([item[\"fixations_binary\"] for item in batch])),\n",
    "        \"fixations_blurred\": torch.tensor([item[\"fixations_blurred\"] for item in batch])\n",
    "    }\n",
    "    if \"features\" in batch[0].keys():\n",
    "        batch_data[\"features\"] = torch.tensor([item[\"features\"] for item in batch], dtype=torch.float)\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z-roY4tNDweN"
   },
   "source": [
    "**Compute center bias**\n",
    "\n",
    "People tend to look towards the center of the image. This is both due to a bias in our expectations as well as the tendency of photographers to place the interesting objects in the center of the photo. Thus, in the fixation data there is a bias towards the center of the image. We incorporate this bias into our model explicitly. \n",
    "\n",
    "**TO DO:** To compute the center bias, sum all the binary fixation maps, normalize and apply a Gaussian filter to smooth the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U3-itd9hDweP"
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "dataset_train = ImageDataset(train_data)\n",
    "fixations_binary = dataset_train.data['fixations_binary']\n",
    "\n",
    "#TODO\n",
    "center_bias = ...    # (should be 1 x 256 x 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IhM7Cp_eD2pD"
   },
   "source": [
    "Show the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QH9fDjZ0Dwel"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(center_bias[0])\n",
    "cb = plt.colorbar().set_label('log density')\n",
    "plt.title('Center bias');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9A67ttHkDwey"
   },
   "source": [
    "Save/Load center bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yjrrv34aDwez"
   },
   "outputs": [],
   "source": [
    "center_bias_file = os.path.join(main_path, 'center_bias.npy')\n",
    "np.save(center_bias_file, center_bias)\n",
    "#center_bias = np.load(center_bias_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cjX1ydxkDwdl"
   },
   "source": [
    "**Extract feature maps**\n",
    "\n",
    "Load the VGG19 model pretrained on ImageNet. Use it to process the images in the test dataset and extract the feature maps from the Conv2d layer vgg19.features[30]. You should save them in a Numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "012ps7UXDwdn"
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "vgg = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "geyuZ0QvDwdv"
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "features = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RdnFKp25Dwd0"
   },
   "source": [
    "**Save the features to a file**\n",
    "\n",
    "Processing the images can take a few minutes. However, you only have to do it once. Thus it is a good idea to save the features to a file that you can load again later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fT7B2fZcDwd3"
   },
   "outputs": [],
   "source": [
    "features_file = os.path.join(main_path, 'features_train.npy')\n",
    "np.save(features_file, features)\n",
    "#features = np.load(features_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GTsDojrc8QMi"
   },
   "source": [
    "Add the features to the dataset and define a data loader for training our model. Feel free to change the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gxaZPaakDwd_"
   },
   "outputs": [],
   "source": [
    "dataset_train = ImageDataset(train_data, features=features)\n",
    "\n",
    "data_loader_train = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    shuffle=True,\n",
    "    batch_size=64,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iswz9l74DwdQ"
   },
   "source": [
    "**Define the model**\n",
    "\n",
    "Now we need to define the model we want to train. Two parts are necessary:\n",
    "\n",
    "1. Readout Network\n",
    "The readout network gets the feature maps we extracted from VGG19 as input. The network could be of arbitrary complexity but in our case we want to keep it simple: it contains only one layer that takes a linear combination of the features at each location (using 1x1 convolution).\n",
    "\n",
    "2. Finalizer\n",
    "The finalizer takes the output of the readout network and scales it up to the size of our images (256x256). It then applies a Gaussian filter to blur the result of the upscaling and adds the center bias (don't forget to give a learnable weight to the center bias). Finally it is normalized using a (log) softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0mJyxnt-ZZEz"
   },
   "outputs": [],
   "source": [
    "class GaussianBlur2d(nn.Module):\n",
    "    \"\"\"\n",
    "    This class applies a Gaussian filter to a tensor. You should \n",
    "    use it *after* upscaling to image size (256x256). The kernel\n",
    "    size and the standard deviation are fixed and you don't need\n",
    "    to change them.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.kernel_size = 121\n",
    "        self.sigma = 20\n",
    "        self.pad = 60\n",
    "\n",
    "        grid = torch.arange(float(self.kernel_size)) - self.pad\n",
    "\n",
    "        kernel_shape_x = [1] * 4\n",
    "        kernel_shape_x[3] = self.kernel_size\n",
    "        grid_x = grid.view(kernel_shape_x)\n",
    "        self.kernel_x = torch.exp(-0.5 * (grid_x / self.sigma) ** 2)#.cuda()\n",
    "        self.kernel_x = self.kernel_x / self.kernel_x.sum()\n",
    "\n",
    "        kernel_shape_y = [1] * 4\n",
    "        kernel_shape_y[2] = self.kernel_size\n",
    "        grid_y = grid.view(kernel_shape_y)\n",
    "        self.kernel_y = torch.exp(-0.5 * (grid_y / self.sigma) ** 2)#.cuda()\n",
    "        self.kernel_y = self.kernel_y / self.kernel_y.sum()\n",
    "\n",
    "        \n",
    "    def forward(self, tensor):\n",
    "        \"\"\"Applies the gaussian filter to the given tensor\"\"\"\n",
    "        out = F.conv1d(tensor, self.kernel_x, padding=self.pad)\n",
    "        out = F.conv1d(out, self.kernel_y)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ARp9R0LfDwdg"
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "readout_network = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lz08zcSlDwdT"
   },
   "outputs": [],
   "source": [
    "class Finalizer(nn.Module):\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_vyoYLsYDwdY"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, readout_network, finalizer):\n",
    "        super().__init__()\n",
    "        self.readout_network = readout_network\n",
    "        self.finalizer = finalizer\n",
    "        \n",
    "    def forward(self, x, centerbias):\n",
    "        x = self.readout_network(x)\n",
    "        x = self.finalizer(x, centerbias)\n",
    "        return x\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        self.readout_network.train(mode=mode)\n",
    "        self.finalizer.train(mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gI6b6ou6DwfZ"
   },
   "outputs": [],
   "source": [
    "finalizer = Finalizer()\n",
    "model = Model(readout_network, finalizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7ZPvRojDweF"
   },
   "source": [
    "**Define the metrics**\n",
    "\n",
    "Here we use two different metrics, one for training and one for evaluating the performance of our model. For the training, we use negative log-likelihood (NLL). Implement a function that calculates the log-likelihood of a fixation mask under the model prediction. \n",
    "\n",
    "*Hint: You can convert a sparse tensor t to a dense one using t.to_dense().*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8V-0MZIqDweG"
   },
   "outputs": [],
   "source": [
    "def log_likelihood(log_density, fixation_mask):\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UL8H7BQmD2qT"
   },
   "source": [
    "For evaluation, we want to use the *normalized scanpath saliency* (NSS). NSS is the average of the response values at human eye positions in a model’s saliency map SM(x) that has been normalized to have zero mean and unit standard deviation. That is, we take the values\n",
    "\n",
    "$Z_{\\mathrm{SM}} = \\frac{\\mathrm{SM}(x) - \\mu}{\\sigma}$,\n",
    "\n",
    "where $\\mu$ and $\\sigma$ are the mean and the standard deviation of the saliency map, respectively, and we take the average over all fixations:\n",
    "\n",
    "$\\mathrm{NSS} = \\frac{1}{M} \\sum_{k=1}^M Z_{\\mathrm{SM}}(x_f(k))$. \n",
    "\n",
    "Implement this as a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XFHB0vZjD2qW"
   },
   "outputs": [],
   "source": [
    "def nss(log_density, fixation_mask):\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LxHRN-_jDwfW"
   },
   "source": [
    "**Train the model**\n",
    "\n",
    "Now iterate through the dataloader to train the model on the test dataset. You can use the Adam optimizer with default parameters. You should print the NLL and the NSS of the test dataset after each training epoch to monitor the progress. Train for at least 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "arHuTrY7Dwff"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, centerbias):\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b5b0vzH1Dwfn"
   },
   "outputs": [],
   "source": [
    "%time train_model(model, data_loader_train, torch.tensor(center_bias, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HQIhz6ZyDwgT"
   },
   "source": [
    "You can save the model to disk if you want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KrTmXfFADwgV"
   },
   "outputs": [],
   "source": [
    "model_file = os.path.join(main_path, 'model_12012020.torch')\n",
    "torch.save(model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UTG-8AiRDwgc"
   },
   "source": [
    "Use this to load a model that you've saved previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kzpGMzAZDwge"
   },
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(model_file)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WNiAgyyHDwgu"
   },
   "source": [
    "**Inspect the results**\n",
    "\n",
    "Let's find out how well your model performs. Use the given plot function to visualize the saliency maps for\n",
    "1. the three elements of the test dataset with the *highest NSS* and\n",
    "2. the three elements of the test dataset with the *lowest NSS*.\n",
    "\n",
    "Print the respective NSS values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ASqxjv6QDwgw"
   },
   "outputs": [],
   "source": [
    "test_data = preprocess_data(images_files['test'], fixation_maps_binary_files['test'], fixation_maps_blurred_files['test'], 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w3avBGJIWuVk"
   },
   "outputs": [],
   "source": [
    "test_data_file = os.path.join(main_path, 'test_data.npy')\n",
    "np.save(test_data_file, test_data)\n",
    "#test_data = np.load(test_data_file, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "trv1sxKTD2q4"
   },
   "source": [
    "Here you need to extract the feature maps for the test images. It works the same way as before for the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9e6wO-QwDwg8"
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "features_test = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y__YcYKpDwhI"
   },
   "outputs": [],
   "source": [
    "features_test_file = os.path.join(main_path, 'features_test.npy')\n",
    "np.save(features_test_file, features_test)\n",
    "#features_test = np.load(features_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LsH_8cqyDwhM"
   },
   "outputs": [],
   "source": [
    "dataset_test = ImageDataset(test_data, features=features_test)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    shuffle=False,\n",
    "    batch_size=1,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4MabQFX0DwhR"
   },
   "source": [
    "**Display test images with highest scores**\n",
    "\n",
    "For the images with the highest scores the predicted saliency map should look very similar to the observed fixation map.\n",
    "\n",
    "Here is the best example from our reference implementation:\n",
    "\n",
    "![alt text](https://i.imgur.com/qgUQG2W.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b1CDHZ55DwhS"
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "scores = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s5619dGTDwhh"
   },
   "outputs": [],
   "source": [
    "def plot(image, fixations, blurred_map, prediction):\n",
    "    fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3)\n",
    "    ax1.axis('off')\n",
    "    ax1.imshow(image)\n",
    "    ax1.set_title('original image')\n",
    "    ax2.axis('off')\n",
    "    ax2.imshow(blurred_map, cmap='viridis_r')\n",
    "    ax2.set_title('blurred fixation map')\n",
    "    ax3.axis('off')\n",
    "    ax3.imshow(prediction, cmap='viridis_r')\n",
    "    ax3.set_title('model prediction')\n",
    "    ax4.axis('off')\n",
    "    ax4.imshow(image, alpha=0.5)\n",
    "    ax4.scatter(fixations[1], fixations[0], s=10, marker='.', c='b')\n",
    "    ax4.set_title('overlayed with discrete fixations')\n",
    "    ax5.axis('off')\n",
    "    ax5.imshow(image)\n",
    "    ax5.imshow(blurred_map, alpha=0.5, cmap='viridis_r')\n",
    "    ax5.set_title('overlayed with original image')\n",
    "    ax6.axis('off')\n",
    "    ax6.imshow(image)\n",
    "    ax6.imshow(prediction, alpha=0.5, cmap='viridis_r')\n",
    "    ax6.set_title('overlayed with original image')\n",
    "    #fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "46tYUyxfDwhk"
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "#plot(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rZvytUxhZgFU"
   },
   "source": [
    "**Display test images with lowest scores**\n",
    "\n",
    "Here is an example from our reference implementation for which the prediction is poor:\n",
    "\n",
    "![alt text](https://i.imgur.com/g4rD2O1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wvcfQA9fDwis"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tX3jzqeFZlfB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DL19_Homework4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
